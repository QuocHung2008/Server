import warnings
warnings.filterwarnings("ignore", message="pkg_resources is deprecated as an API")

import os, hashlib, json, cv2, numpy as np
from typing import Dict, List
import face_recognition
import sys

KNOWN_FACES_DIR = "known_faces"
ENCODINGS_FILE = "encodings.npz"
META_FILE = "encodings_meta.json"

# ‚úÖ GLOBAL FLAG ƒë·ªÉ tr√°nh recursive call
_ENCODING_IN_PROGRESS = set()

def compute_known_faces_hash(known_dir: str = KNOWN_FACES_DIR) -> str:
    entries = []
    for root, _, files in os.walk(known_dir):
        for f in sorted(files):
            path = os.path.join(root, f)
            try:
                entries.append(f"{os.path.relpath(path, known_dir)}|{os.path.getmtime(path)}")
            except OSError:
                pass
    return hashlib.sha1("\n".join(entries).encode("utf-8")).hexdigest()

def get_largest_face_location(face_locations):
    """Tr·∫£ v·ªÅ location c·ªßa khu√¥n m·∫∑t l·ªõn nh·∫•t"""
    if not face_locations:
        return None
    
    if len(face_locations) == 1:
        return face_locations[0]
    
    areas = [(loc[2] - loc[0]) * (loc[1] - loc[3]) for loc in face_locations]
    largest_idx = np.argmax(areas)
    
    if len(face_locations) > 1:
        print(f"    üéØ T√¨m th·∫•y {len(face_locations)} m·∫∑t, ch·ªçn m·∫∑t l·ªõn nh·∫•t (di·ªán t√≠ch: {areas[largest_idx]} px)")
    
    return face_locations[largest_idx]

def build_encodings_for_class(class_dir: str) -> Dict[str, List]:
    """Build encodings v·ªõi protection ch·ªëng l·∫∑p v√¥ h·∫°n"""
    
    # ‚úÖ PROTECTION 1: Ki·ªÉm tra ƒëang encode hay ch∆∞a
    abs_class_dir = os.path.abspath(class_dir)
    if abs_class_dir in _ENCODING_IN_PROGRESS:
        print(f"‚ö†Ô∏è C·∫¢NH B√ÅO: {class_dir} ƒëang ƒë∆∞·ª£c encode, b·ªè qua ƒë·ªÉ tr√°nh l·∫∑p!")
        return {"encodings": [], "names": []}
    
    _ENCODING_IN_PROGRESS.add(abs_class_dir)
    
    try:
        return _build_encodings_internal(class_dir, force_rebuild=False)
    finally:
        # ‚úÖ PROTECTION 2: Lu√¥n remove kh·ªèi set khi xong
        _ENCODING_IN_PROGRESS.discard(abs_class_dir)

def build_encodings_for_class_force(class_dir: str) -> Dict[str, List]:
    abs_class_dir = os.path.abspath(class_dir)
    if abs_class_dir in _ENCODING_IN_PROGRESS:
        print(f"‚ö†Ô∏è C·∫¢NH B√ÅO: {class_dir} ƒëang ƒë∆∞·ª£c encode, b·ªè qua ƒë·ªÉ tr√°nh l·∫∑p!")
        return {"encodings": [], "names": []}
    _ENCODING_IN_PROGRESS.add(abs_class_dir)
    try:
        return _build_encodings_internal(class_dir, force_rebuild=True)
    finally:
        _ENCODING_IN_PROGRESS.discard(abs_class_dir)

def _build_encodings_internal(class_dir: str, force_rebuild: bool) -> Dict[str, List]:
    """H√†m encode th·ª±c s·ª± (internal)"""
    
    known_dir = os.path.join(class_dir, "known_faces")
    encodings_file = os.path.join(class_dir, "encodings.npz")
    legacy_encodings_file = os.path.join(class_dir, "encodings.pkl")
    meta_file = os.path.join(class_dir, "encodings_meta.json")

    print(f"\n{'='*70}")
    print(f"üîß B·∫ÆT ƒê·∫¶U ENCODE: {os.path.basename(class_dir)}")
    print(f"{'='*70}")

    if not os.path.exists(known_dir):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {known_dir}")
        return {"encodings": [], "names": []}

    current_hash = compute_known_faces_hash(known_dir)
    stored_hash = None
    if os.path.exists(meta_file):
        try:
            with open(meta_file, "r", encoding="utf-8") as f:
                stored_hash = (json.load(f) or {}).get("hash")
        except Exception:
            stored_hash = None

    if not force_rebuild and stored_hash and stored_hash == current_hash and os.path.exists(encodings_file):
        try:
            loaded = np.load(encodings_file, allow_pickle=False)
            encs = [e for e in loaded["encodings"]]
            names = [str(n) for n in loaded["names"]]
            print(f"üì¶ Encoding ƒë√£ up-to-date: {len(set(names))} ng∆∞·ªùi, {len(names)} ·∫£nh")
            return {"encodings": encs, "names": names}
        except Exception:
            pass

    if force_rebuild:
        print("‚ôªÔ∏è Force rebuild encodings (b·ªè qua cache c≈©)")
    else:
        print("‚ôªÔ∏è Detected changes in known_faces, rebuild encodings")

    if not os.path.exists(known_dir):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {known_dir}")
        return {"encodings": [], "names": []}
    
    people = sorted([d for d in os.listdir(known_dir) if os.path.isdir(os.path.join(known_dir, d))])

    updated_encodings, updated_names = [], []
    
    # Encode t·ª´ng ng∆∞·ªùi
    for idx, name in enumerate(people, 1):
        person_dir = os.path.join(known_dir, name)
        
        print(f"\n[{idx}/{len(people)}] üë§ {name}")
        
        # L·ªçc file ·∫£nh h·ª£p l·ªá
        image_files = sorted([f for f in os.listdir(person_dir) 
                            if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])
        
        if not image_files:
            print(f"   ‚ö†Ô∏è Kh√¥ng c√≥ file ·∫£nh!")
            processed_names.add(name)  # ‚úÖ V·∫´n ƒë√°nh d·∫•u
            continue
        
        print(f"   üì∑ T√¨m th·∫•y {len(image_files)} file ·∫£nh")
        
        person_encodings = []
        
        for img_idx, imgname in enumerate(image_files, 1):
            img_path = os.path.join(person_dir, imgname)
            
            try:
                # ƒê·ªçc ·∫£nh ƒë∆°n gi·∫£n
                image = face_recognition.load_image_file(img_path)
                
                # Resize n·∫øu qu√° l·ªõn
                h, w = image.shape[:2]
                if w > 1600:
                    scale = 1600 / w
                    image = cv2.resize(image, (1600, int(h * scale)))
                    print(f"   [{img_idx}/{len(image_files)}] üìê {imgname} (resized)")
                else:
                    print(f"   [{img_idx}/{len(image_files)}] üîç {imgname}")
                
                # T√¨m m·∫∑t v·ªõi HOG (nhanh, ·ªïn ƒë·ªãnh)
                face_locations = face_recognition.face_locations(image, model="hog")
                
                if not face_locations:
                    print(f"      ‚ö†Ô∏è Kh√¥ng c√≥ m·∫∑t")
                    continue
                
                # L·∫•y m·∫∑t l·ªõn nh·∫•t
                largest_face = get_largest_face_location(face_locations)
                
                # Encode v·ªõi c√†i ƒë·∫∑t nh·∫π
                encs = face_recognition.face_encodings(
                    image, 
                    known_face_locations=[largest_face],
                    num_jitters=2  # Gi·∫£m xu·ªëng 2 cho nhanh
                )
                
                if not encs:
                    print(f"      ‚ö†Ô∏è Kh√¥ng encode ƒë∆∞·ª£c")
                    continue
                
                person_encodings.append(encs[0])
                print(f"      ‚úÖ OK")
                
            except Exception as e:
                print(f"      ‚ùå L·ªói: {str(e)[:50]}")
        
        # L∆∞u k·∫øt qu·∫£
        if person_encodings:
            for enc in person_encodings:
                updated_encodings.append(enc)
                updated_names.append(name)
            print(f"   ‚úÖ Th√†nh c√¥ng: {len(person_encodings)}/{len(image_files)} ·∫£nh")
        else:
            print(f"   ‚ö†Ô∏è Kh√¥ng c√≥ ·∫£nh n√†o h·ª£p l·ªá!")
        
    # L∆∞u file
    print(f"\nüíæ ƒêang l∆∞u v√†o {encodings_file}...")

    enc_arr = np.asarray(updated_encodings, dtype=np.float64)
    name_arr = np.asarray(updated_names, dtype=str)
    np.savez_compressed(encodings_file, encodings=enc_arr, names=name_arr)
    
    unique_people = len(set(updated_names))
    total_images = len(updated_names)
    
    with open(meta_file, "w") as f:
        json.dump({
            "hash": current_hash,
            "count": total_images,
            "unique_people": unique_people
        }, f)

    print(f"\n{'='*70}")
    print(f"‚úÖ HO√ÄN T·∫§T: {os.path.basename(class_dir)}")
    print(f"{'='*70}")
    print(f"üë• T·ªïng s·ªë ng∆∞·ªùi: {unique_people}")
    print(f"üì∑ T·ªïng s·ªë ·∫£nh: {total_images}")
    if unique_people > 0:
        print(f"üìä Trung b√¨nh: {total_images / unique_people:.1f} ·∫£nh/ng∆∞·ªùi")
    print(f"{'='*70}\n")
    
    return {"encodings": updated_encodings, "names": updated_names}

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python encode_known_faces_fixed.py <class_dir>")
        print("Example: python encode_known_faces_fixed.py classes/Lop10A")
        print("\nHo·∫∑c encode t·∫•t c·∫£ l·ªõp:")
        print("python encode_known_faces_fixed.py classes")
        sys.exit(1)
    
    path = sys.argv[1]
    
    # N·∫øu truy·ªÅn v√†o th∆∞ m·ª•c "classes" ‚Üí encode t·∫•t c·∫£ l·ªõp
    if os.path.isdir(path) and os.path.basename(path) == "classes":
        print(f"üîÑ S·∫Ω encode t·∫•t c·∫£ l·ªõp trong: {path}\n")
        
        class_dirs = []
        for name in sorted(os.listdir(path)):
            class_dir = os.path.join(path, name)
            if os.path.isdir(class_dir):
                known_faces = os.path.join(class_dir, "known_faces")
                if os.path.exists(known_faces):
                    class_dirs.append(class_dir)
        
        if not class_dirs:
            print("‚ùå Kh√¥ng t√¨m th·∫•y l·ªõp n√†o c√≥ th∆∞ m·ª•c known_faces!")
            sys.exit(1)
        
        print(f"T√¨m th·∫•y {len(class_dirs)} l·ªõp\n")
        
        for i, class_dir in enumerate(class_dirs, 1):
            print(f"\n{'#'*70}")
            print(f"# [{i}/{len(class_dirs)}] {os.path.basename(class_dir)}")
            print(f"{'#'*70}")
            build_encodings_for_class(class_dir)
    
    # Encode 1 l·ªõp c·ª• th·ªÉ
    else:
        build_encodings_for_class(path)
